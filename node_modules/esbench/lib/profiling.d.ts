import { Awaitable } from "@kaciras/utilities/browser";
import { LogLevel } from "./host/context.js";
import { RunSuiteOption } from "./runner.js";
import { BenchCase, NormalizedSuite, Scene } from "./suite.js";
export type LogType = Exclude<LogLevel, "off">;
/**
 * Calling this function always requires `await` in order to send the message as soon as possible.
 */
export type LogHandler = (message: string | undefined, level: LogType) => Awaitable<any>;
/**
 * Metrics of a benchmark case measured by profilers.
 *
 * The value of any metric can be absent, reporters should be able to handle this.
 */
export type Metrics = Record<string, number | number[] | string | undefined>;
/**
 * Bench cases in the scene with their metrics.
 */
export type SceneResult = Record<string, Metrics>;
export declare enum MetricAnalysis {
    /**
     * There is no analyze performed to the metric. This is the default value.
     */
    None = 0,
    /**
     * Reporters should show diff & ratio with another result if present for the metric.
     * The metric value must be a number or an array of number with at least 1 element.
     */
    Compare = 1,
    /**
     * Reporters should display statistical indicators (stdDev, percentiles...) for the metric.
     * The metric value must be an array of number with at least 1 element.
     *
     * Setting this value will also apply `MetricAnalysis.Compare`.
     */
    Statistics = 2
}
export interface MetricMeta {
    /**
     * Property name of the metric in Metrics.
     */
    key: string;
    /**
     * Specific the format when this metric displayed as text (numeric metric only).
     * If not defined, the value will be converted using `.toString`.
     *
     * @example
     * "{duration.ms}" // The metric is millisecond and should be formatted as duration.
     * "{number} ops/s" // The value 2000 will be formatted to "2K ops/s".
     */
    format?: string;
    /**
     * Control which metrics can be derived from this (numeric metric only).
     *
     * @default MetricAnalysis.None
     */
    analysis?: MetricAnalysis;
    /**
     * Does a smaller value of the metric mean better performance?
     * This option must be set if `analysis` is not `None`.
     */
    lowerIsBetter?: boolean;
}
export interface Note {
    type: "info" | "warn";
    text: string;
    caseId?: number;
}
export interface Profiler {
    /**
     * Called on each `ProfilingContext.run` (`runSuite` invokes it once).
     * This is the recommended hook to add descriptions of metrics.
     */
    onStart?: (ctx: ProfilingContext) => Awaitable<void>;
    /**
     * Called on each scene (after `setup` of the suite).
     */
    onScene?: (ctx: ProfilingContext, scene: Scene) => Awaitable<void>;
    /**
     * Called for each case. In there you can add metrics as properties to `metrics`.
     */
    onCase?: (ctx: ProfilingContext, case_: BenchCase, metrics: Metrics) => Awaitable<void>;
    /**
     * Called at the end of `ProfilingContext.run`.
     */
    onFinish?: (ctx: ProfilingContext) => Awaitable<void>;
}
export declare class ProfilingContext {
    /**
     * Result for each case in each scene.
     */
    readonly scenes: SceneResult[];
    /**
     * Notes collected from the profiling.
     *
     * @see ProfilingContext.note
     */
    readonly notes: Note[];
    /**
     * Descriptions of metrics.
     *
     * @see ProfilingContext.defineMetric
     */
    readonly meta: Record<string, MetricMeta>;
    readonly suite: NormalizedSuite;
    readonly profilers: Profiler[];
    readonly pattern: RegExp;
    readonly logHandler: LogHandler;
    private hasRun;
    private caseIndex;
    constructor(suite: NormalizedSuite, profilers: Profiler[], options: RunSuiteOption);
    /**
     * Profiler should add description for each metric that need to be reported.
     *
     * Values in the case metrics without descriptions will not be shown in the report,
     * but they will still be serialized.
     *
     * @param description The description of the metric.
     */
    defineMetric(description: MetricMeta): void;
    /**
     * Using this method will generate warnings, which are logs with log level "warn".
     */
    warn(message?: string): any;
    /**
     * Generate an "info" log. As these logs are displayed by default, use them for information
     * that is not a warning but makes sense to display to all users.
     */
    info(message?: string): any;
    /**
     * Add a note to result, it will print a log and displayed in the report.
     *
     * The different between notes and logs is that
     * notes are only relevant to the result, while logs can record anything.
     *
     * @param type Type of the note, "info" or "warn".
     * @param text The message of this note.
     * @param case_ The case associated with this note.
     */
    note(type: "info" | "warn", text: string, case_?: BenchCase): any;
    /**
     * Create a new ProfilingContext for the same suite.
     *
     * Profilers & options are not inherited.
     */
    newWorkflow(profilers: Profiler[], options?: RunSuiteOption): ProfilingContext;
    /**
     * Run the profiling, the result is saved at `scenes`, `notes` and `meta` properties.
     *
     * A ProfilingContext instance can only be run once.
     */
    run(): Promise<void>;
    private runScene;
    private runHooks;
}
