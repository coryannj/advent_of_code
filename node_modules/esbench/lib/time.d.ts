import { Awaitable } from "@kaciras/utilities/browser";
import { BenchCase } from "./suite.js";
import { Metrics, Profiler, ProfilingContext } from "./profiling.js";
interface Iterator {
    invocations: number;
    loops: number;
    calls: number;
    iterate: () => Awaitable<number>;
}
export interface TimingOptions {
    /**
     * How many target iterations should be performed. The value must >= 1.
     *
     * @default 10
     */
    samples?: number;
    /**
     * How many warmup iterations should be performed. The value can be 0, which disables warmup.
     *
     * @default 5
     */
    warmup?: number;
    /**
     * Number of iterations for one sample, the larger the result the more accurate and slower it is.
     *
     * If is a duration string, it used by Pilot stage to estimate the number of invocations
     * such that the duration of a sample (including iteration hooks) is close to the given value.
     *
     * If the value is a number, it must be a multiple of `unrollFactor`, the load will call the given
     * number of times for each sample. It's better for benchmarks which doesn't have a steady state
     * and the performance distribution is tricky.
     *
     * If the value is less than the `unrollFactor`, `unrollFactor` will be forced to be the same.
     *
     * @default "1s"
     */
    iterations?: number | string;
    /**
     * How many times the benchmark method will be invoked per one iteration of a generated loop.
     *
     * It can be ignored if `iterations` is a duration and the workload takes long time.
     *
     * @default 16
     */
    unrollFactor?: number;
    /**
     * Specifies if the overhead should be evaluated (Idle runs) and it's average value
     * subtracted from every result. Very important for nano-benchmarks.
     *
     * @default true
     */
    evaluateOverhead?: boolean;
}
/**
 * A tool used for accurately measure the execution time of a benchmark case.
 *
 * @example
 * const profiler = {
 *     async onCase(ctx, case_, metrics) {
 *         const measurement = new ExecutionTimeMeasurement(ctx, case_);
 *         await measurement.run();
 *         ctx.info(`Samples: [${measurement.values.join(", ")}]`);
 *     },
 * }
 */
export declare class ExecutionTimeMeasurement {
    private readonly ctx;
    private readonly benchCase;
    private readonly options;
    private stage?;
    private stageRuns;
    /**
     * After running, it is the number of runs needed for the benchmark case
     * to reach the `options.iterations` time.
     *
     * If `options.iterations` is a number, the value is equal to it.
     */
    invocations: number;
    /**
     * After running, it is the number of the loop is unrolled during the measurement.
     */
    unrollCalls: number;
    /**
     * Samples of benchmark function running time.
     */
    values: number[];
    constructor(ctx: ProfilingContext, case_: BenchCase, options?: TimingOptions);
    static normalize(options?: TimingOptions): Required<TimingOptions>;
    run(): Promise<void>;
    subtractOverhead(iterator: Iterator): Promise<void>;
    measureOverhead({ calls, loops }: Iterator): Promise<number[]>;
    estimate(target: string): Promise<Iterator>;
    measure(name: string, iterator: Iterator): Promise<number[]>;
    private logStageRun;
}
export interface TimeProfilerOptions extends TimingOptions {
    /**
     * Measure throughput (ops/<unit>) instead of time (time/op).
     * The value can be a duration unit.
     *
     * @example
     * defineSuite({ timing: { throughput: "s" } });
     * | No. |   Name |   throughput |
     * | --: | -----: | -----------: |
     * |   0 | object | 14.39M ops/s |
     * |   1 |    map | 17.32M ops/s |
     */
    throughput?: string;
}
export declare class TimeProfiler implements Profiler {
    private readonly throughput?;
    private readonly config?;
    constructor(config?: TimeProfilerOptions);
    onStart(ctx: ProfilingContext): Promise<void>;
    onCase(ctx: ProfilingContext, case_: BenchCase, metrics: Metrics): Promise<void>;
}
export {};
